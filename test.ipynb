{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dd340a0-b56a-4d14-a44c-c40e77638ac9",
   "metadata": {},
   "source": [
    "## req "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09005515-aba5-4f6f-a40a-b889999954ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (1.0.16)\n",
      "Requirement already satisfied: langchain in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: llama-index in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: langchain_experimental in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (0.3.4)\n",
      "Requirement already satisfied: langchain_openai in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (0.3.29)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from chromadb) (1.3.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from chromadb) (2.11.7)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from chromadb) (1.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from chromadb) (2.1.3)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from chromadb) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from chromadb) (1.22.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from chromadb) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from chromadb) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from chromadb) (1.36.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from chromadb) (0.21.4)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from chromadb) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from chromadb) (1.74.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from chromadb) (0.9.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from chromadb) (33.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from chromadb) (9.0.0)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from chromadb) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from chromadb) (3.11.1)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from chromadb) (4.23.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.7 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb) (2025.4.26)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langchain) (0.3.74)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langchain) (0.4.13)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langchain) (2.0.39)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: llama-index-cli<0.6,>=0.5.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-index) (0.5.0)\n",
      "Requirement already satisfied: llama-index-core<0.14,>=0.13.1 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-index) (0.13.1)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.6,>=0.5.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-index) (0.5.0)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-index) (0.9.0)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.6,>=0.5.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-index) (0.5.0)\n",
      "Requirement already satisfied: llama-index-readers-file<0.6,>=0.5.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-index) (0.5.0)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-index) (0.5.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.1->llama-index) (3.11.10)\n",
      "Requirement already satisfied: aiosqlite in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.1->llama-index) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.2.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.1->llama-index) (2.2.0)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.1->llama-index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.1->llama-index) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.1->llama-index) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.1->llama-index) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.1->llama-index) (2025.3.2)\n",
      "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.1->llama-index) (1.2.0)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.1->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.1->llama-index) (3.4.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.1->llama-index) (11.1.0)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.1->llama-index) (4.3.7)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.1->llama-index) (80.9.0)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.1->llama-index) (0.10.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.1->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.1->llama-index) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.1->llama-index) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.1->llama-index) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.1->llama-index) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.1->llama-index) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.1->llama-index) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.1->llama-index) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.1->llama-index) (1.18.0)\n",
      "Requirement already satisfied: griffe in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.1->llama-index) (1.10.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.1->llama-index) (3.1.6)\n",
      "Requirement already satisfied: openai>=1.1.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.99.1)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (4.12.3)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (0.7.1)\n",
      "Requirement already satisfied: pandas<2.3.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.2.3)\n",
      "Requirement already satisfied: pypdf<6,>=5.1.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (5.9.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (0.0.26)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.5)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.14,>=0.13.1->llama-index) (0.4.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (4.7.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (0.10.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.3.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2025.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from tqdm>=4.65.0->chromadb) (0.4.6)\n",
      "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langchain_experimental) (0.3.27)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.6.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.4.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from dataclasses-json->llama-index-core<0.14,>=0.13.1->llama-index) (3.26.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.1.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.14,>=0.13.1->llama-index) (1.0.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from tiktoken>=0.7.0->llama-index-core<0.14,>=0.13.1->llama-index) (2024.11.6)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.22.3)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.40.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: llama-cloud==0.1.35 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.35)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.54)\n",
      "Requirement already satisfied: llama-cloud-services>=0.6.54 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.54)\n",
      "Requirement already satisfied: click<9,>=8.1.7 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from llama-cloud-services>=0.6.54->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.36.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.57b0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.34.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.17.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.1->llama-index) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U chromadb langchain llama-index langchain_experimental langchain_openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ea445aa-5c73-4f5b-a2a1-c09915e46f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vreddy_quantum-i\\Desktop\\Symptoms_checker\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())  # This shows your current working directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23fcf0ae-ba49-4205-8842-78793f79e117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (1.26.3)\n",
      "Requirement already satisfied: langchain in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langchain) (0.3.74)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langchain) (0.4.13)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langchain) (2.0.39)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.12.2)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.7.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "!pip install pymupdf langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa347e26-2a22-4435-a737-bf9de7790222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: openai in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (1.99.1)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (0.10.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langchain) (0.3.74)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langchain) (0.4.13)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langchain) (2.0.39)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.12.2)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.55.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.34.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.4.26)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n",
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n",
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n",
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain openai tiktoken\n",
    "!pip install faiss-cpu  # or faiss-gpu if needed\n",
    "!pip install sentence-transformers\n",
    "!pip install chromadb  # optional if using Chroma instead of FAISS\n",
    "!pip install pymupdf  # for PDF reading\n",
    "!pip install pypdf    # optional PDF reading alternative\n",
    "!pip install python-dotenv  # to manage API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3dd855d5-83d1-43bf-b70f-6a62ef960186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2935a53-7742-4e33-806b-bbead846307c",
   "metadata": {},
   "source": [
    "## code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26f5cc7c-f5f8-4b75-8e45-c97fa58bbf69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (1.11.0.post1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from faiss-cpu) (2.1.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\vreddy_quantum-i\\anaconda3\\lib\\site-packages (from faiss-cpu) (24.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "674607e9-b685-4600-a791-b1f527316737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text(path):\n",
    "    with fitz.open(path) as doc:  # ensures proper file closing\n",
    "        return \"\\n\".join([page.get_text() for page in doc])\n",
    "\n",
    "# ✅ Make sure the filename has the correct extension\n",
    "text = extract_text(r\"C:\\Users\\vreddy_quantum-i\\Downloads\\DSM-5_filtered.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c1a8fce-b6d2-4466-b360-05d59f90996c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunks: 4175\n",
      "SECTION II\n",
      "Diagnostic Criteria and Codes\n",
      "Neurodevelopmental Disorders\n",
      "Schizophrenia Spectrum and Other Psychotic Disorders\n",
      "Bipolar and Related Disorders\n",
      "Depressive Disorders\n",
      "Anxiety Disorders\n",
      "Obsessive-Compulsive and Related Disorders\n",
      "Trauma- and Stressor-Related Disorders\n",
      "Dissociative Disorders\n",
      "Som\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function print(*args, sep=' ', end='\\n', file=None, flush=False)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Convert to a Langchain Document\n",
    "doc = Document(page_content=text, metadata={\"source\": \"1100_pages.pdf\"})\n",
    "\n",
    "# Initialize splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,         # number of characters per chunk\n",
    "    chunk_overlap=200        # slight overlap between chunks\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents([doc])\n",
    "print(f\"Total Chunks: {len(chunks)}\")\n",
    "print(chunks[0].page_content[:300])  # Preview first chunk\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68d6c35-2278-4a3f-abb1-294efdc8eb00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "124a1b9b-7d4e-4281-8b78-09472f9dcb72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d015b51ef9d45e885944cd85b66158e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'Document' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1569\u001b[0m, in \u001b[0;36mSentenceTransformer.tokenize\u001b[1;34m(self, texts, **kwargs)\u001b[0m\n\u001b[0;32m   1568\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtokenize(texts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:280\u001b[0m, in \u001b[0;36mTransformer.tokenize\u001b[1;34m(self, texts, padding)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text_tuple \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[1;32m--> 280\u001b[0m     batch1\u001b[38;5;241m.\u001b[39mappend(text_tuple[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    281\u001b[0m     batch2\u001b[38;5;241m.\u001b[39mappend(text_tuple[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Document' object is not subscriptable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 17\u001b[0m\n\u001b[0;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Assume your chunks are in a list called `chunks`\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Example: chunks = [\"Depression is defined as...\", \"Schizophrenia criteria include...\"]\u001b[39;00m\n\u001b[0;32m     14\u001b[0m  \n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Embed chunks\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(chunks, show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Create FAISS index\u001b[39;00m\n\u001b[0;32m     21\u001b[0m dimension \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1019\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[0m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start_index \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sentences), batch_size, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatches\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[0;32m   1018\u001b[0m     sentences_batch \u001b[38;5;241m=\u001b[39m sentences_sorted[start_index : start_index \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m-> 1019\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize(sentences_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1020\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1021\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1571\u001b[0m, in \u001b[0;36mSentenceTransformer.tokenize\u001b[1;34m(self, texts, **kwargs)\u001b[0m\n\u001b[0;32m   1569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtokenize(texts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m-> 1571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtokenize(texts)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:280\u001b[0m, in \u001b[0;36mTransformer.tokenize\u001b[1;34m(self, texts, padding)\u001b[0m\n\u001b[0;32m    278\u001b[0m batch1, batch2 \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text_tuple \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[1;32m--> 280\u001b[0m     batch1\u001b[38;5;241m.\u001b[39mappend(text_tuple[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    281\u001b[0m     batch2\u001b[38;5;241m.\u001b[39mappend(text_tuple[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    282\u001b[0m to_tokenize \u001b[38;5;241m=\u001b[39m [batch1, batch2]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Document' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import faiss\n",
    "\n",
    "import pickle\n",
    " \n",
    "# Load model\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    " \n",
    "# Assume your chunks are in a list called `chunks`\n",
    "\n",
    "# Example: chunks = [\"Depression is defined as...\", \"Schizophrenia criteria include...\"]\n",
    " \n",
    "# Embed chunks\n",
    "\n",
    "embeddings = model.encode(chunks, show_progress_bar=True)\n",
    " \n",
    "# Create FAISS index\n",
    "\n",
    "dimension = embeddings.shape[1]\n",
    "\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "index.add(embeddings)\n",
    " \n",
    "# Save index and chunks\n",
    "\n",
    "faiss.write_index(index, \"dsm5_index.faiss\")\n",
    "\n",
    "with open(\"dsm5_chunks.pkl\", \"wb\") as f:\n",
    "\n",
    "    pickle.dump(chunks, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1afaf9e-8a2a-4511-b074-0301023135b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    " \n",
    "# Load index and chunks\n",
    "\n",
    "index = faiss.read_index(\"dsm5_index.faiss\")\n",
    "\n",
    "with open(\"dsm5_chunks.pkl\", \"rb\") as f:\n",
    "\n",
    "    chunks = pickle.load(f)\n",
    " \n",
    "# User query\n",
    "\n",
    "query = \"Patient reports low mood, fatigue, and feelings of worthlessness.\"\n",
    " \n",
    "# Embed the query\n",
    "\n",
    "query_embedding = model.encode([query])\n",
    " \n",
    "# Search top 5 similar\n",
    "\n",
    "D, I = index.search(np.array(query_embedding), k=5)\n",
    " \n",
    "# Retrieve matched chunks\n",
    "\n",
    "retrieved_chunks = [chunks[i] for i in I[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c5c2bc-6f2c-4a7e-8b68-6dfb1e032e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Step 1: Extract text from PDF\n",
    "def extract_text(path):\n",
    "    try:\n",
    "        with fitz.open(path) as doc:\n",
    "            return \"\\n\".join([page.get_text() for page in doc])\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF: {e}\")\n",
    "        return None\n",
    "\n",
    "# Step 2: Prepare documents and chunk them\n",
    "def prepare_documents(pdf_path):\n",
    "    text = extract_text(pdf_path)\n",
    "    if not text:\n",
    "        raise ValueError(\"Failed to extract text from PDF\")\n",
    "    \n",
    "    doc = Document(page_content=text, metadata={\"source\": pdf_path})\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = splitter.split_documents([doc])\n",
    "    print(f\"Total Chunks: {len(chunks)}\")\n",
    "    print(f\"First chunk preview: {chunks[0].page_content[:300]}\")\n",
    "    return chunks\n",
    "\n",
    "# Step 3: Create and save embeddings\n",
    "def create_embeddings(chunks, model_name=\"all-MiniLM-L6-v2\", index_file=\"dsm5_index.faiss\", chunks_file=\"dsm5_chunks.pkl\"):\n",
    "    try:\n",
    "        model = SentenceTransformer(model_name)\n",
    "        chunk_texts = [chunk.page_content for chunk in chunks]\n",
    "        embeddings = model.encode(chunk_texts, show_progress_bar=True)\n",
    "        \n",
    "        dimension = embeddings.shape[1]\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        index.add(embeddings)\n",
    "        \n",
    "        faiss.write_index(index, index_file)\n",
    "        with open(chunks_file, \"wb\") as f:\n",
    "            pickle.dump(chunks, f)\n",
    "        print(f\"Saved index to {index_file} and chunks to {chunks_file}\")\n",
    "        return index, chunks\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating embeddings: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Step 4: Load index and chunks\n",
    "def load_index_and_chunks(index_file=\"dsm5_index.faiss\", chunks_file=\"dsm5_chunks.pkl\"):\n",
    "    if os.path.exists(index_file) and os.path.exists(chunks_file):\n",
    "        try:\n",
    "            index = faiss.read_index(index_file)\n",
    "            with open(chunks_file, \"rb\") as f:\n",
    "                chunks = pickle.load(f)\n",
    "            print(f\"Loaded index from {index_file} and chunks from {chunks_file}\")\n",
    "            return index, chunks\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading index or chunks: {e}\")\n",
    "            return None, None\n",
    "    return None, None\n",
    "\n",
    "# Step 5: Retrieve relevant chunks\n",
    "def retrieve_chunks(query, index, chunks, model, top_k=3):\n",
    "    try:\n",
    "        query_embedding = model.encode([query])[0]\n",
    "        distances, indices = index.search(np.array([query_embedding]), top_k)\n",
    "        return [chunks[i].page_content for i in indices[0]]\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving chunks: {e}\")\n",
    "        return []\n",
    "\n",
    "# Step 6: Generate response\n",
    "def generate_response(query, relevant_chunks, model_name=\"distilgpt2\"):\n",
    "    try:\n",
    "        generator = pipeline(\"text-generation\", model=model_name, device=0 if torch.cuda.is_available() else -1)\n",
    "        context = \"\\n\".join(relevant_chunks)\n",
    "        prompt = f\"Question: {query}\\nContext: {context}\\nAnswer in a concise and accurate manner:\"\n",
    "        response = generator(prompt, max_length=200, num_return_sequences=1, truncation=True)[0][\"generated_text\"]\n",
    "        # Extract only the answer part\n",
    "        answer = response.split(\"Answer:\")[-1].strip() if \"Answer:\" in response else response.strip()\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return \"Sorry, I couldn't generate a response.\"\n",
    "\n",
    "# Main function\n",
    "def main(pdf_path, query):\n",
    "    # Prepare documents\n",
    "    chunks = prepare_documents(pdf_path)\n",
    "    if not chunks:\n",
    "        return \"Failed to prepare documents.\"\n",
    "    \n",
    "    # Load or create embeddings\n",
    "    index_file, chunks_file = \"dsm5_index.faiss\", \"dsm5_chunks.pkl\"\n",
    "    index, chunks = load_index_and_chunks(index_file, chunks_file)\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    if index is None or chunks is None:\n",
    "        print(\"Creating new embeddings...\")\n",
    "        index, chunks = create_embeddings(chunks, model_name=\"all-MiniLM-L6-v2\", index_file=index_file, chunks_file=chunks_file)\n",
    "        if index is None or chunks is None:\n",
    "            return \"Failed to create embeddings.\"\n",
    "    \n",
    "    # Retrieve and generate\n",
    "    relevant_chunks = retrieve_chunks(query, index, chunks, model, top_k=3)\n",
    "    if not relevant_chunks:\n",
    "        return \"No relevant information found.\"\n",
    "    \n",
    "    response = generate_response(query, relevant_chunks)\n",
    "    return response\n",
    "\n",
    "# Command-line interface for testing\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = r\"C:\\Users\\vreddy_quantum-i\\Downloads\\DSM-5_filtered.pdf\"\n",
    "    \n",
    "    # Verify PDF exists\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"PDF file not found at {pdf_path}\")\n",
    "    else:\n",
    "        while True:\n",
    "            query = input(\"Enter your question (or 'quit' to exit): \")\n",
    "            if query.lower() == \"quit\":\n",
    "                break\n",
    "            response = main(pdf_path, query)\n",
    "            print(f\"\\nQuery: {query}\")\n",
    "            print(f\"Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e01990-5d7b-4584-aa87-9ba6e22ded92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee8b4a60-5dee-4295-b5b3-55d3bddf563b",
   "metadata": {},
   "source": [
    "### test code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fdecc87-21a9-4b42-af26-8c4d6554b2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All imports successful!\n",
      "✅ PDFSymptomCheckerRAG class defined successfully!\n",
      "Loading embedding model: all-MiniLM-L6-v2 on cpu\n",
      "✅ Initialized RAG system with all-MiniLM-L6-v2 on cpu\n",
      "🚀 RAG system initialized!\n",
      "🏗️ Building Knowledge Base...\n",
      "📄 Extracting text from: C:\\Users\\vreddy_quantum-i\\Downloads\\DSM-5_filtered.pdf\n",
      "✅ Successfully extracted text using PyMuPDF: 3097485 characters\n",
      "🧹 Cleaned text length: 3092736 characters\n",
      "✂️ Creating text chunks...\n",
      "✅ Created 3451 chunks\n",
      "🧮 Creating embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c47a5c083184b9b8bf7e9865651b4ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created embeddings: (3451, 384)\n",
      "✅ Knowledge base built successfully!\n",
      "📚 Knowledge base built from PDF!\n",
      "💾 Knowledge base saved to: dsm5_symptom_checker.pkl\n",
      "💾 Knowledge base saved to: dsm5_symptom_checker.pkl\n",
      "🔍 Query: I feel anxious and have panic attacks\n",
      "📊 Confidence: 0.579\n",
      "📚 Number of sources: 3\n",
      "\n",
      "📝 Response:\n",
      "⚠️ MEDICAL DISCLAIMER: This information is for educational purposes only and is based on clinical literature. Always consult with a qualified healthcare professional for proper diagnosis and treatment.\n",
      "\n",
      "Based on your symptoms, here's relevant information from the clinical literature:\n",
      "\n",
      "📄 Source 1 (Similarity: 0.588):\n",
      ". The course of panic disorder typically is complicated by a range of other disorders, in Temperamental. Environmental. particular other anxiety disorders, depressive disorders, and substance use disorders (see section Comorbidity for this disorder). African American adults have been reported to have a more chronic course of panic disorder compared with non-Latinx White adults, possibly because of...\n",
      "\n",
      "📄 Source 2 (Similarity: 0.581):\n",
      ". Individuals who report fears of dying in their panic attacks tend to have more severe presentations of panic disorder (e.g., panic attacks involving more symptoms). The maladaptive changes in behavior represent attempts to minimize or avoid panic attacks or their consequences. Examples include avoiding physical exertion, reorganizing daily life to ensure that help is available in the event of a ...\n",
      "\n",
      "📄 Source 3 (Similarity: 0.567):\n",
      ". The specifier with panic attacks may therefore be used for panic attacks that occur in the context of any anxiety disorder, as well as other mental disorders (e.g., depressive disorders, posttraumatic stress disorder). Individuals with agoraphobia are fearful and anxious in many different situations, and the diagnostic criteria require symptoms in two or more of the following: using public trans...\n",
      "\n",
      "🔍 Next Steps:\n",
      "• Document all symptoms with duration and severity\n",
      "• Consult with a mental health professional for proper assessment\n",
      "• Consider keeping a symptom diary\n",
      "• Seek immediate help if experiencing severe distress\n",
      "🧪 Testing multiple queries:\n",
      "\n",
      "1. Query: 'I have trouble concentrating and feel restless'\n",
      "   📊 Confidence: 0.526\n",
      "   📚 Sources: 2\n",
      "------------------------------------------------------------\n",
      "2. Query: 'I experience mood swings and feel very sad'\n",
      "   📊 Confidence: 0.576\n",
      "   📚 Sources: 2\n",
      "------------------------------------------------------------\n",
      "3. Query: 'I have obsessive thoughts and compulsive behaviors'\n",
      "   📊 Confidence: 0.705\n",
      "   📚 Sources: 2\n",
      "------------------------------------------------------------\n",
      "4. Query: 'I hear voices and feel paranoid'\n",
      "   📊 Confidence: 0.533\n",
      "   📚 Sources: 2\n",
      "------------------------------------------------------------\n",
      "5. Query: 'I have flashbacks and nightmares after a trauma'\n",
      "   📊 Confidence: 0.644\n",
      "   📚 Sources: 2\n",
      "------------------------------------------------------------\n",
      "✅ Helper functions defined!\n",
      "📊 Knowledge Base Statistics:\n",
      "• Total chunks: 3451\n",
      "• Embedding dimensions: 384\n",
      "• Model: all-MiniLM-L6-v2\n",
      "• Device: CPU\n",
      "• Chunk distribution: {'symptoms': 2380, 'diagnosis': 650, 'general': 174, 'treatment': 247}\n",
      "\n",
      "📋 Sample chunks:\n",
      "1. [symptoms] SECTION II Diagnostic Criteria and Codes Neurodevelopmental Disorders Schizophrenia Spectrum and Oth...\n",
      "2. [symptoms] . For each mental disorder, the diagnostic criteria are followed by descriptive text to assist in di...\n",
      "3. [symptoms] . Other Conditions That May Be a Focus of Clinical Attention includes conditions and psychosocial or...\n",
      "✅ All cells ready!\n",
      "💡 Instructions:\n",
      "1. Update the PDF path in Cell 5 to match your file location\n",
      "2. Run cells sequentially from Cell 1 to Cell 10\n",
      "3. Use ask_symptom_checker('your symptoms') for quick queries\n",
      "4. For future sessions, use the code in Cell 11 to load existing knowledge base\n"
     ]
    }
   ],
   "source": [
    "# COMPLETE JUPYTER NOTEBOOK SOLUTION\n",
    "# Copy each cell block into separate Jupyter notebook cells\n",
    "\n",
    "# ===== CELL 1: Install Required Packages =====\n",
    "# Run this first if packages are not installed\n",
    "\"\"\"\n",
    "!pip install pandas numpy scikit-learn sentence-transformers torch\n",
    "!pip install PyPDF2 PyMuPDF langchain nltk\n",
    "\"\"\"\n",
    "\n",
    "# ===== CELL 2: Imports and Setup =====\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PDF processing\n",
    "import PyPDF2\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# ML and embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Text processing\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"✅ All imports successful!\")\n",
    "\n",
    "# ===== CELL 3: Define PDFSymptomCheckerRAG Class =====\n",
    "class PDFSymptomCheckerRAG:\n",
    "    def __init__(self, embedding_model='all-MiniLM-L6-v2', use_gpu=False):\n",
    "        \"\"\"\n",
    "        Initialize the RAG system\n",
    "        \n",
    "        Args:\n",
    "            embedding_model: Sentence transformer model to use\n",
    "            use_gpu: Whether to use GPU for embeddings (if available)\n",
    "        \"\"\"\n",
    "        self.embedding_model_name = embedding_model\n",
    "        self.use_gpu = use_gpu and torch.cuda.is_available()\n",
    "        \n",
    "        # Initialize sentence transformer\n",
    "        device = 'cuda' if self.use_gpu else 'cpu'\n",
    "        print(f\"Loading embedding model: {embedding_model} on {device}\")\n",
    "        self.embedder = SentenceTransformer(embedding_model, device=device)\n",
    "        \n",
    "        # Initialize text splitter for chunking\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        )\n",
    "        \n",
    "        # Storage\n",
    "        self.chunks = []\n",
    "        self.chunk_embeddings = None\n",
    "        self.chunk_metadata = []\n",
    "        self.is_trained = False\n",
    "        \n",
    "        print(f\"✅ Initialized RAG system with {embedding_model} on {device}\")\n",
    "    \n",
    "    def extract_text_from_pdf(self, pdf_path: str, method='pymupdf') -> str:\n",
    "        \"\"\"Extract text from PDF file\"\"\"\n",
    "        print(f\"📄 Extracting text from: {pdf_path}\")\n",
    "        \n",
    "        if not os.path.exists(pdf_path):\n",
    "            raise FileNotFoundError(f\"PDF file not found: {pdf_path}\")\n",
    "        \n",
    "        text = \"\"\n",
    "        \n",
    "        if method == 'pymupdf':\n",
    "            try:\n",
    "                # PyMuPDF method - better text extraction\n",
    "                doc = fitz.open(pdf_path)\n",
    "                for page_num in range(len(doc)):\n",
    "                    page = doc.load_page(page_num)\n",
    "                    text += page.get_text()\n",
    "                doc.close()\n",
    "                print(f\"✅ Successfully extracted text using PyMuPDF: {len(text)} characters\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ PyMuPDF failed: {e}. Trying PyPDF2...\")\n",
    "                method = 'pypdf2'\n",
    "        \n",
    "        if method == 'pypdf2':\n",
    "            try:\n",
    "                # PyPDF2 method - fallback\n",
    "                with open(pdf_path, 'rb') as file:\n",
    "                    pdf_reader = PyPDF2.PdfReader(file)\n",
    "                    for page_num in range(len(pdf_reader.pages)):\n",
    "                        page = pdf_reader.pages[page_num]\n",
    "                        text += page.extract_text()\n",
    "                print(f\"✅ Successfully extracted text using PyPDF2: {len(text)} characters\")\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"❌ Both PDF extraction methods failed: {e}\")\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and preprocess extracted text\"\"\"\n",
    "        # Remove excessive whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Remove page numbers and headers/footers (basic patterns)\n",
    "        text = re.sub(r'\\n\\d+\\n', '\\n', text)\n",
    "        text = re.sub(r'\\nPage \\d+\\n', '\\n', text)\n",
    "        \n",
    "        # Remove special characters but keep medical terminology\n",
    "        text = re.sub(r'[^\\w\\s\\.\\,\\;\\:\\-\\(\\)]', ' ', text)\n",
    "        \n",
    "        # Remove excessive spaces\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def create_chunks(self, text: str, custom_chunk_size: Optional[int] = None, \n",
    "                     custom_overlap: Optional[int] = None) -> List[Dict]:\n",
    "        \"\"\"Split text into chunks with metadata\"\"\"\n",
    "        print(\"✂️ Creating text chunks...\")\n",
    "        \n",
    "        # Update text splitter if custom parameters provided\n",
    "        if custom_chunk_size or custom_overlap:\n",
    "            self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=custom_chunk_size or 1000,\n",
    "                chunk_overlap=custom_overlap or 200,\n",
    "                length_function=len,\n",
    "                separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "            )\n",
    "        \n",
    "        # Split text into chunks\n",
    "        raw_chunks = self.text_splitter.split_text(text)\n",
    "        \n",
    "        chunks_with_metadata = []\n",
    "        for i, chunk in enumerate(raw_chunks):\n",
    "            if len(chunk.strip()) > 50:  # Filter out very short chunks\n",
    "                chunk_data = {\n",
    "                    'text': chunk.strip(),\n",
    "                    'chunk_id': i,\n",
    "                    'length': len(chunk),\n",
    "                    'start_char': text.find(chunk),\n",
    "                    'type': self._classify_chunk(chunk)\n",
    "                }\n",
    "                chunks_with_metadata.append(chunk_data)\n",
    "        \n",
    "        print(f\"✅ Created {len(chunks_with_metadata)} chunks\")\n",
    "        return chunks_with_metadata\n",
    "    \n",
    "    def _classify_chunk(self, chunk: str) -> str:\n",
    "        \"\"\"Classify chunk type based on content\"\"\"\n",
    "        chunk_lower = chunk.lower()\n",
    "        \n",
    "        if any(word in chunk_lower for word in ['symptom', 'symptoms', 'present', 'criteria']):\n",
    "            return 'symptoms'\n",
    "        elif any(word in chunk_lower for word in ['treatment', 'therapy', 'medication', 'intervention']):\n",
    "            return 'treatment'\n",
    "        elif any(word in chunk_lower for word in ['diagnosis', 'diagnostic', 'condition', 'disorder']):\n",
    "            return 'diagnosis'\n",
    "        else:\n",
    "            return 'general'\n",
    "    \n",
    "    def create_embeddings(self, chunks: List[Dict]) -> np.ndarray:\n",
    "        \"\"\"Create embeddings for text chunks\"\"\"\n",
    "        print(\"🧮 Creating embeddings...\")\n",
    "        \n",
    "        texts = [chunk['text'] for chunk in chunks]\n",
    "        \n",
    "        # Create embeddings in batches to manage memory\n",
    "        batch_size = 32\n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            batch_embeddings = self.embedder.encode(\n",
    "                batch_texts,\n",
    "                batch_size=batch_size,\n",
    "                show_progress_bar=True if i == 0 else False,\n",
    "                convert_to_numpy=True\n",
    "            )\n",
    "            all_embeddings.append(batch_embeddings)\n",
    "        \n",
    "        embeddings = np.vstack(all_embeddings)\n",
    "        print(f\"✅ Created embeddings: {embeddings.shape}\")\n",
    "        \n",
    "        return embeddings\n",
    "    \n",
    "    def build_knowledge_base(self, pdf_path: str, chunk_size: int = 1000, \n",
    "                           chunk_overlap: int = 200):\n",
    "        \"\"\"Complete pipeline: PDF -> Text -> Chunks -> Embeddings\"\"\"\n",
    "        print(\"🏗️ Building Knowledge Base...\")\n",
    "        \n",
    "        # 1. Extract text from PDF\n",
    "        raw_text = self.extract_text_from_pdf(pdf_path)\n",
    "        \n",
    "        # 2. Clean text\n",
    "        cleaned_text = self.clean_text(raw_text)\n",
    "        print(f\"🧹 Cleaned text length: {len(cleaned_text)} characters\")\n",
    "        \n",
    "        # 3. Create chunks\n",
    "        self.chunks = self.create_chunks(cleaned_text, chunk_size, chunk_overlap)\n",
    "        self.chunk_metadata = [chunk for chunk in self.chunks]\n",
    "        \n",
    "        # 4. Create embeddings\n",
    "        self.chunk_embeddings = self.create_embeddings(self.chunks)\n",
    "        \n",
    "        self.is_trained = True\n",
    "        print(\"✅ Knowledge base built successfully!\")\n",
    "    \n",
    "    def save_knowledge_base(self, filepath: str):\n",
    "        \"\"\"Save the complete knowledge base\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Knowledge base not built. Please build it first.\")\n",
    "        \n",
    "        kb_data = {\n",
    "            'chunks': self.chunks,\n",
    "            'chunk_embeddings': self.chunk_embeddings,\n",
    "            'chunk_metadata': self.chunk_metadata,\n",
    "            'embedding_model': self.embedding_model_name,\n",
    "            'embedder_config': {\n",
    "                'model_name': self.embedding_model_name,\n",
    "                'max_seq_length': self.embedder.max_seq_length\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(kb_data, f)\n",
    "        print(f\"💾 Knowledge base saved to: {filepath}\")\n",
    "    \n",
    "    def load_knowledge_base(self, filepath: str):\n",
    "        \"\"\"Load a pre-built knowledge base\"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            kb_data = pickle.load(f)\n",
    "        \n",
    "        self.chunks = kb_data['chunks']\n",
    "        self.chunk_embeddings = kb_data['chunk_embeddings']\n",
    "        self.chunk_metadata = kb_data['chunk_metadata']\n",
    "        self.is_trained = True\n",
    "        \n",
    "        print(f\"📚 Knowledge base loaded from: {filepath}\")\n",
    "        print(f\"📊 Loaded {len(self.chunks)} chunks with embeddings\")\n",
    "    \n",
    "    def search_similar_chunks(self, query: str, top_k: int = 5, \n",
    "                            similarity_threshold: float = 0.3) -> List[Dict]:\n",
    "        \"\"\"Search for similar chunks using semantic similarity\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Knowledge base not built. Please build or load it first.\")\n",
    "        \n",
    "        # Create query embedding\n",
    "        query_embedding = self.embedder.encode([query], convert_to_numpy=True)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = cosine_similarity(query_embedding, self.chunk_embeddings)[0]\n",
    "        \n",
    "        # Get top-k results above threshold\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            if similarities[idx] >= similarity_threshold:\n",
    "                result = {\n",
    "                    'chunk_id': idx,\n",
    "                    'text': self.chunks[idx]['text'],\n",
    "                    'metadata': self.chunk_metadata[idx],\n",
    "                    'similarity_score': float(similarities[idx])\n",
    "                }\n",
    "                results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_symptom_response(self, query: str, top_k: int = 3) -> Dict:\n",
    "        \"\"\"Generate response for symptom query\"\"\"\n",
    "        # Search for relevant chunks\n",
    "        relevant_chunks = self.search_similar_chunks(query, top_k)\n",
    "        \n",
    "        if not relevant_chunks:\n",
    "            return {\n",
    "                'response': \"I couldn't find relevant information for your symptoms in the knowledge base. Please consult with a healthcare professional for proper diagnosis.\",\n",
    "                'sources': [],\n",
    "                'confidence': 0.0\n",
    "            }\n",
    "        \n",
    "        # Generate response\n",
    "        response_parts = []\n",
    "        \n",
    "        # Medical disclaimer\n",
    "        response_parts.append(\"⚠️ MEDICAL DISCLAIMER: This information is for educational purposes only and is based on clinical literature. Always consult with a qualified healthcare professional for proper diagnosis and treatment.\")\n",
    "        response_parts.append(\"\")\n",
    "        \n",
    "        # Main response\n",
    "        response_parts.append(\"Based on your symptoms, here's relevant information from the clinical literature:\")\n",
    "        response_parts.append(\"\")\n",
    "        \n",
    "        # Add relevant chunks\n",
    "        for i, chunk in enumerate(relevant_chunks, 1):\n",
    "            chunk_text = chunk['text']\n",
    "            # Truncate very long chunks\n",
    "            if len(chunk_text) > 400:\n",
    "                chunk_text = chunk_text[:400] + \"...\"\n",
    "            \n",
    "            response_parts.append(f\"📄 Source {i} (Similarity: {chunk['similarity_score']:.3f}):\")\n",
    "            response_parts.append(chunk_text)\n",
    "            response_parts.append(\"\")\n",
    "        \n",
    "        # Recommendations\n",
    "        response_parts.append(\"🔍 Next Steps:\")\n",
    "        response_parts.append(\"• Document all symptoms with duration and severity\")\n",
    "        response_parts.append(\"• Consult with a mental health professional for proper assessment\")\n",
    "        response_parts.append(\"• Consider keeping a symptom diary\")\n",
    "        response_parts.append(\"• Seek immediate help if experiencing severe distress\")\n",
    "        \n",
    "        avg_confidence = np.mean([chunk['similarity_score'] for chunk in relevant_chunks])\n",
    "        \n",
    "        return {\n",
    "            'response': '\\n'.join(response_parts),\n",
    "            'sources': relevant_chunks,\n",
    "            'confidence': float(avg_confidence),\n",
    "            'num_sources': len(relevant_chunks)\n",
    "        }\n",
    "\n",
    "print(\"✅ PDFSymptomCheckerRAG class defined successfully!\")\n",
    "\n",
    "# ===== CELL 4: Initialize System =====\n",
    "# Initialize the RAG system\n",
    "rag_system = PDFSymptomCheckerRAG(\n",
    "    embedding_model='all-MiniLM-L6-v2',  # Good balance of speed and quality\n",
    "    use_gpu=False  # Set to True if you have NVIDIA GPU with CUDA\n",
    ")\n",
    "\n",
    "print(\"🚀 RAG system initialized!\")\n",
    "\n",
    "# ===== CELL 5: Process Your PDF =====\n",
    "# Update this path to your actual PDF location\n",
    "pdf_path = r\"C:\\Users\\vreddy_quantum-i\\Downloads\\DSM-5_filtered.pdf\"\n",
    "\n",
    "try:\n",
    "    # Build the knowledge base\n",
    "    rag_system.build_knowledge_base(\n",
    "        pdf_path=pdf_path,\n",
    "        chunk_size=1200,    # Larger chunks for clinical content\n",
    "        chunk_overlap=300   # Good overlap for context preservation\n",
    "    )\n",
    "    print(\"📚 Knowledge base built from PDF!\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ PDF file not found at: {pdf_path}\")\n",
    "    print(\"Please update the path to match your actual file location.\")\n",
    "    print(\"Example: pdf_path = r'C:\\\\path\\\\to\\\\your\\\\DSM-5_filtered.pdf'\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error processing PDF: {e}\")\n",
    "\n",
    "# ===== CELL 6: Save Knowledge Base =====\n",
    "if rag_system.is_trained:\n",
    "    # Save for future use (so you don't have to rebuild)\n",
    "    kb_file = 'dsm5_symptom_checker.pkl'\n",
    "    rag_system.save_knowledge_base(kb_file)\n",
    "    print(f\"💾 Knowledge base saved to: {kb_file}\")\n",
    "else:\n",
    "    print(\"⚠️ Knowledge base not built. Please fix PDF path and run Cell 5 first.\")\n",
    "\n",
    "# ===== CELL 7: Test Single Query =====\n",
    "if rag_system.is_trained:\n",
    "    # Test with a sample symptom query\n",
    "    test_query = \"I feel anxious and have panic attacks\"\n",
    "    response = rag_system.generate_symptom_response(test_query, top_k=3)\n",
    "\n",
    "    print(f\"🔍 Query: {test_query}\")\n",
    "    print(f\"📊 Confidence: {response['confidence']:.3f}\")\n",
    "    print(f\"📚 Number of sources: {response['num_sources']}\")\n",
    "    print(f\"\\n📝 Response:\\n{response['response']}\")\n",
    "else:\n",
    "    print(\"⚠️ Knowledge base not ready. Please build it first.\")\n",
    "\n",
    "# ===== CELL 8: Test Multiple Queries =====\n",
    "if rag_system.is_trained:\n",
    "    # Test with multiple queries\n",
    "    test_queries = [\n",
    "        \"I have trouble concentrating and feel restless\",\n",
    "        \"I experience mood swings and feel very sad\",\n",
    "        \"I have obsessive thoughts and compulsive behaviors\",\n",
    "        \"I hear voices and feel paranoid\",\n",
    "        \"I have flashbacks and nightmares after a trauma\"\n",
    "    ]\n",
    "\n",
    "    print(\"🧪 Testing multiple queries:\\n\")\n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        response = rag_system.generate_symptom_response(query, top_k=2)\n",
    "        print(f\"{i}. Query: '{query}'\")\n",
    "        print(f\"   📊 Confidence: {response['confidence']:.3f}\")\n",
    "        print(f\"   📚 Sources: {response['num_sources']}\")\n",
    "        print(\"-\" * 60)\n",
    "else:\n",
    "    print(\"⚠️ Knowledge base not ready. Please build it first.\")\n",
    "\n",
    "# ===== CELL 9: Custom Query Function =====\n",
    "def ask_symptom_checker(symptoms, top_k=3):\n",
    "    \"\"\"\n",
    "    Convenience function for quick queries\n",
    "    \n",
    "    Args:\n",
    "        symptoms (str): Description of symptoms\n",
    "        top_k (int): Number of sources to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        Formatted response\n",
    "    \"\"\"\n",
    "    if not rag_system.is_trained:\n",
    "        return \"❌ Knowledge base not loaded. Please run the setup cells first.\"\n",
    "    \n",
    "    response = rag_system.generate_symptom_response(symptoms, top_k)\n",
    "    \n",
    "    print(f\"🔍 Query: {symptoms}\")\n",
    "    print(f\"📊 Confidence: {response['confidence']:.3f}\")\n",
    "    print(f\"📚 Sources: {response['num_sources']}\")\n",
    "    print(f\"\\n{response['response']}\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"✅ Helper functions defined!\")\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "# ask_symptom_checker(\"I feel depressed and have no energy\")\n",
    "\n",
    "# ===== CELL 10: System Statistics =====\n",
    "if rag_system.is_trained:\n",
    "    print(\"📊 Knowledge Base Statistics:\")\n",
    "    print(f\"• Total chunks: {len(rag_system.chunks)}\")\n",
    "    print(f\"• Embedding dimensions: {rag_system.chunk_embeddings.shape[1]}\")\n",
    "    print(f\"• Model: {rag_system.embedding_model_name}\")\n",
    "    print(f\"• Device: {'GPU' if rag_system.use_gpu else 'CPU'}\")\n",
    "    \n",
    "    # Show chunk types\n",
    "    chunk_types = {}\n",
    "    for chunk in rag_system.chunks:\n",
    "        chunk_type = chunk.get('type', 'general')\n",
    "        chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1\n",
    "    \n",
    "    print(f\"• Chunk distribution: {chunk_types}\")\n",
    "    \n",
    "    # Show some sample chunks\n",
    "    print(f\"\\n📋 Sample chunks:\")\n",
    "    for i, chunk in enumerate(rag_system.chunks[:3], 1):\n",
    "        preview = chunk['text'][:100] + \"...\" if len(chunk['text']) > 100 else chunk['text']\n",
    "        print(f\"{i}. [{chunk['type']}] {preview}\")\n",
    "else:\n",
    "    print(\"⚠️ Knowledge base not built yet. Please run the setup cells first.\")\n",
    "\n",
    "# ===== CELL 11: Load Existing Knowledge Base (For Future Sessions) =====\n",
    "\"\"\"\n",
    "# For future sessions, you can load pre-built knowledge base:\n",
    "\n",
    "# Initialize new system\n",
    "rag_system_loaded = PDFSymptomCheckerRAG()\n",
    "\n",
    "# Load existing knowledge base\n",
    "rag_system_loaded.load_knowledge_base('dsm5_symptom_checker.pkl')\n",
    "\n",
    "# Now you can immediately start querying\n",
    "response = rag_system_loaded.generate_symptom_response(\"your symptoms here\")\n",
    "print(response['response'])\n",
    "\"\"\"\n",
    "\n",
    "print(\"✅ All cells ready!\")\n",
    "print(\"💡 Instructions:\")\n",
    "print(\"1. Update the PDF path in Cell 5 to match your file location\")\n",
    "print(\"2. Run cells sequentially from Cell 1 to Cell 10\")\n",
    "print(\"3. Use ask_symptom_checker('your symptoms') for quick queries\")\n",
    "print(\"4. For future sessions, use the code in Cell 11 to load existing knowledge base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f086d15b-4f56-4552-a543-8a1d93f23ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
